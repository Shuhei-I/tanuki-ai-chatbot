import os
import streamlit as st
import tiktoken
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–ï¼’
if "messages" not in st.session_state:
    st.session_state.messages = ConversationBufferWindowMemory(return_messages=True,k=3)

# ãƒšãƒ¼ã‚¸å†…å®¹
        
st.title("ãŸã¬ãä¼šè©±AI")
st.subheader('ã‚ãªãŸã®è¦æœ›ã« :orange[ãŸã¬ã] ãŒç­”ãˆã¾ã™ï¼', divider='rainbow')

messages = st.container(height=300) # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¬„ã®å¤§ãã•

# å…¥åŠ›æ¬„
prompt = st.chat_input("ãªã«ã‹ã—ã‚ƒã¹ã‚Šã‹ã‘ã¦ã‚ˆï¼")
# å…¥åŠ›ã•ã‚ŒãŸã‚‰
if prompt :
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç†
    # å…¥åŠ›å†…å®¹ã‚’st.session_state.messagesã«è¿½åŠ ã—ã€è¡¨è¨˜ã™ã‚‹
    # st.session_state.messages.append({"role":"user","content": prompt})
    messages.chat_message("user").write(prompt)
    
    # è¿”ç­”å‡¦ç†
    with messages.chat_message("ğŸ¦",avatar="statics/images/tanuki.jpg"):
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æº–å‚™
        template = """ã‚ãªãŸã¯ã€ŒãŸã¬ãã€ã¨ã„ã†å¯æ„›ã„ã‚±ãƒ¼ã‚­ã§ã™ã€‚è¨€è‘‰é£ã„ã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ã€èªå°¾ã«ã€ŒãŸã¬ãƒ¼ï¼ã€ã‚’ã¤ã‘ã¾ã™ã€‚
        
        ä¼šè©±å†…å®¹ï¼š{question}
        
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æº–å‚™
        prompt_template = PromptTemplate(
            input_variables=["question"],
            template=template
        )

        # ä¼šè©±å±¥æ­´ã‚’ç”¨ã„ãŸä¼šè©±å¿œç­”
        llm = ChatOpenAI(model_name="gpt-4", temperature = 0)
        conversation = LLMChain(
            llm=llm,
            prompt=prompt_template,
            memory = st.session_state.messages,
            verbose=True
        )

        response = conversation.run(prompt)
        

        st.write(response)
    
    #ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    enc = tiktoken.get_encoding("gpt2")
    input_tokens =  enc.encode(prompt)
    output_tokens = enc.encode(response)
    tokens = input_tokens + output_tokens
    print("--------------------------------------")
    print(f"TokenResult:input({len(input_tokens)}),output({len(output_tokens)}),total({len(tokens)})")
    print("--------------------------------------")

    # ä¼šè©±å†…å®¹ã‚’sessionã®å¤‰æ•°ã«ä¿å­˜
    # st.session_state.messages.save_context({"input": prompt}, {"output": response})
    print(st.session_state.messages.load_memory_variables({}))

st.caption('ã‚ãã¾ã§ã‚‚ :orange[ãŸã¬ãã®æ„è¦‹] ã ã‹ã‚‰çœŸé¢ç›®ã«ã¨ã‚‰ãˆãªã„ã§ã­ï¼')
